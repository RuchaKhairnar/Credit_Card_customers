plot(tree_model)
fancyRpartPlot(tree_model,sub = NA)
#Then predicting the model using test data
pred_tree_model<-predict(tree_model,test_data_aa,type = "class")
table(pred_tree_model,test_data_aa$Attrition_Flag)
##calculating the accuracy.
mean(pred_tree_model==test_data_aa$Attrition_Flag)
confusionMatrix(pred_tree_model,test_AF)
data1a<- train_data_aa[,-21]
data2a<-test_data_aa[,-21]
##Here we use decision tree model
tree_model<-rpart(Attrition_Flag~.,data data1a,method = "class")
##Here we use decision tree model
tree_model<-rpart(Attrition_Flag~.,data= data1a,method = "class")
summary(tree_model)
#Then predicting the model using test data
pred_tree_model<-predict(tree_model,data2a,type = "class")
plot(pred_tree_model)
table(pred_tree_model,test_data_aa$Attrition_Flag)
##calculating the accuracy.
mean(pred_tree_model==test_data_aa$Attrition_Flag)
confusionMatrix(pred_tree_model,test_AF)
##Here we use decision tree model
tree_model<-rpart(Attrition_Flag~.,data =train_data,method = "class")
summary(tree_model)
##Plotting the tree model
plot(tree_model)
fancyRpartPlot(tree_model,sub = NA)
#Then predicting the model using test data
pred_tree_model<-predict(tree_model,test_data,type = "class")
plot(pred_tree_model)
table(pred_tree_model,test_data$Attrition_Flag)
##calculating the accuracy.
mean(pred_tree_model==test_data$Attrition_Flag)
confusionMatrix(pred_tree_model,test_AF)
identical(train_data,train_data_aa)
View(train_data_aa)
View(train_data)
View(test_data)
library(ggthemes)
library(plotfunctions)
library(tidyr)
library(ggplot2)
library(moderndive)
library(skimr)
library(dplyr)
library(tidyverse)
library(readxl)
library(purrr)
library("TeachingDemos")
library("PerformanceAnalytics")
library(corrplot)
library(dummies)
library(caret)
library(generics)
library("e1071")
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(randomForest)
library(effects)
library(e1071)
#drop 3 colnames
data1<-BankChurners[,c(-22,-23,-1)]
colnames(data1)
## Assigning binary values
data1$Attrition_Flag[data1$Attrition_Flag=="Existing Customer"]<-0
data1$Attrition_Flag[data1$Attrition_Flag=="Attrited Customer"]<-1
#Assigning binary values to gender
data1$Gender[data1$Gender=="M"]<-0
data1$Gender[data1$Gender=="F"]<-1
##Naming and changing the class of some of columns
data1<- data1 %>%
summarise("Attrition_Flag"=as.numeric(Attrition_Flag),
Customer_Age,
"Gender"=as.factor(Gender),
"Dependent_count"=as.factor(Dependent_count),
Education_Level,
Marital_Status,
Income_Category,
Card_Category,
Months_on_book,
Total_Relationship_Count,
Months_Inactive_12_mon,
Contacts_Count_12_mon,
Credit_Limit,
Total_Revolving_Bal,
Avg_Open_To_Buy,
Total_Amt_Chng_Q4_Q1,
Total_Trans_Amt,
Total_Trans_Ct,
Total_Ct_Chng_Q4_Q1,
Avg_Utilization_Ratio)
##Adding "sr" in columns
data1$sr<- 1:nrow(data1)
colnames(data1)
#dividing the data into existed and attributed
data_exist<- data1 %>%
filter(Attrition_Flag==0)
data_attrit<- data1 %>%
filter(Attrition_Flag==1)
colnames(data1)
#colnames(data1) <- make.unique(names(data1))
DF<-function(DF, n=0) {
DF[rowSums(is.na(DF)) <= n,]
}
aa1<- data1[sample(data_exist$sr,length(data_exist$sr)*0.6),] %>% DF()
aa2<- data1[sample(data_attrit$sr,length(data_attrit$sr)*0.7),] %>% DF()
train_data_aa<-rbind(aa1,aa2)
aa3<-setdiff(data_exist$sr,aa1$sr)
test_existaa<-data1[aa3,] %>% DF()
aa4<-setdiff(data_attrit$sr,aa2$sr)
test_attritaa<- data1[aa4,] %>% DF()
test_data_aa<- rbind(test_existaa,test_attritaa)
logit_train<- glm(formula = Attrition_Flag ~
Dependent_count +
scale(Avg_Utilization_Ratio) +
scale(Total_Ct_Chng_Q4_Q1) +
scale(Total_Trans_Ct * Total_Trans_Amt) +
scale(Total_Revolving_Bal) +
scale(Months_Inactive_12_mon) +
scale(Contacts_Count_12_mon) +
Gender +
Income_Category +
Marital_Status +
scale(Total_Ct_Chng_Q4_Q1) +
log(Avg_Open_To_Buy) * Credit_Limit,
family = binomial(link = "logit"), data = train_data_aa)
summary(logit_train)
pred_model<- predict(logit_train,newdata = test_data_aa,type = "response")
qplot(pred_model)
plot(pred_model)
test_data_aa$pred[pred_model<=0.5]<-0
test_data_aa$pred[pred_model<=0.5]<-0
test_data_aa$pred[pred_model>0.5]<-1
table(a1=test_data_aa$pred,b=test_data_aa$Attrition_Flag)
## we factorise the data so as to use it in consfusion matrix.
test_pred<-test_data_aa$pred%>% factor()
test_AF<-as.factor(test_data_aa$Attrition_Flag)
confusionMatrix(test_pred,test_AF)
##Calculate accuracy.
mean(test_data$pred==test_data_aa$Attrition_Flag)
##Calculate accuracy.
mean(test_data_aa$pred==test_data_aa$Attrition_Flag)
##Logistic Regression .
logit_train<- glm(formula = Attrition_Flag ~
Dependent_count +
scale(Avg_Utilization_Ratio) +
scale(Total_Ct_Chng_Q4_Q1) +
scale(Total_Trans_Ct * Total_Trans_Amt) +
scale(Total_Revolving_Bal) +
scale(Months_Inactive_12_mon) +
scale(Contacts_Count_12_mon) +
Gender +
Income_Category +
Marital_Status +
scale(Total_Ct_Chng_Q4_Q1) +
log(Avg_Open_To_Buy) * Credit_Limit,
family = binomial(link = "logit"), data = train_data_aa)
summary(logit_train)
pred_model<- predict(logit_train,newdata = test_data_aa,type = "response")
qplot(pred_model)
plot(pred_model)
## Here we define any value less than 0.5 to be predicted as 0 and
##greater than 0.5 to be predicted as 1.
test_data_aa$pred[pred_model<=0.5]<-0
test_data_aa$pred[pred_model>0.5]<-1
table(a1=test_data_aa$pred,b=test_data_aa$Attrition_Flag)
## we factorise the data so as to use it in consfusion matrix.
test_pred<-test_data_aa$pred%>% factor()
test_AF<-as.factor(test_data_aa$Attrition_Flag)
confusionMatrix(test_pred,test_AF)
##Calculate accuracy.
mean(test_data_aa$pred==test_data_aa$Attrition_Flag)
F_meas(test_pred,test_AF)
recall(test_pred,test_AF)
precision(test_pred,test_AF)
##Logistic Regression .
logit_train<- glm(formula = Attrition_Flag ~
Dependent_count +
scale(Avg_Utilization_Ratio) +
scale(Total_Ct_Chng_Q4_Q1) +
scale(Total_Trans_Ct * Total_Trans_Amt) +
scale(Total_Revolving_Bal) +
scale(Months_Inactive_12_mon) +
scale(Contacts_Count_12_mon) +
Gender +
Income_Category +
Marital_Status +
scale(Total_Ct_Chng_Q4_Q1) +
log(Avg_Open_To_Buy) * Credit_Limit,
family = binomial(link = "logit"), data = train_data_aa)
summary(logit_train)
pred_model<- predict(logit_train,newdata = test_data_aa,type = "response")
qplot(pred_model)
plot(pred_model)
## Here we define any value less than 0.5 to be predicted as 0 and
##greater than 0.5 to be predicted as 1.
test_data_aa$pred[pred_model<=0.5]<-0
test_data_aa$pred[pred_model>0.5]<-1
table(a1=test_data_aa$pred,b=test_data_aa$Attrition_Flag)
## we factorise the data so as to use it in consfusion matrix.
test_pred<-test_data_aa$pred%>% factor()
test_AF<-as.factor(test_data_aa$Attrition_Flag)
confusionMatrix(test_pred,test_AF)
##Calculate accuracy.
mean(test_data_aa$pred==test_data_aa$Attrition_Flag)
F_meas(test_pred,test_AF)
recall(test_pred,test_AF)
precision(test_pred,test_AF)
library(ggthemes)
library(plotfunctions)
library(tidyr)
library(ggplot2)
library(moderndive)
library(skimr)
library(dplyr)
library(tidyverse)
library(readxl)
library(purrr)
library("TeachingDemos")
library("PerformanceAnalytics")
library(corrplot)
library(dummies)
library(caret)
library(generics)
library("e1071")
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(randomForest)
library(effects)
library(e1071)
#drop 3 colnames
data1<-BankChurners[,c(-22,-23,-1)]
colnames(data1)
## Assigning binary values
data1$Attrition_Flag[data1$Attrition_Flag=="Existing Customer"]<-0
data1$Attrition_Flag[data1$Attrition_Flag=="Attrited Customer"]<-1
#Assigning binary values to gender
data1$Gender[data1$Gender=="M"]<-0
data1$Gender[data1$Gender=="F"]<-1
##Naming and changing the class of some of columns
data1<- data1 %>%
summarise("Attrition_Flag"=as.numeric(Attrition_Flag),
Customer_Age,
"Gender"=as.factor(Gender),
"Dependent_count"=as.factor(Dependent_count),
Education_Level,
Marital_Status,
Income_Category,
Card_Category,
Months_on_book,
Total_Relationship_Count,
Months_Inactive_12_mon,
Contacts_Count_12_mon,
Credit_Limit,
Total_Revolving_Bal,
Avg_Open_To_Buy,
Total_Amt_Chng_Q4_Q1,
Total_Trans_Amt,
Total_Trans_Ct,
Total_Ct_Chng_Q4_Q1,
Avg_Utilization_Ratio)
##Adding "sr" in columns
data1$sr<- 1:nrow(data1)
colnames(data1)
#dividing the data into existed and attributed
data_exist<- data1 %>%
filter(Attrition_Flag==0)
data_attrit<- data1 %>%
filter(Attrition_Flag==1)
colnames(data1)
#colnames(data1) <- make.unique(names(data1))
DF<-function(DF, n=0) {
DF[rowSums(is.na(DF)) <= n,]
}
set.seed(123)
aa1<- data1[sample(data_exist$sr,length(data_exist$sr)*0.6),] %>% DF()
aa2<- data1[sample(data_attrit$sr,length(data_attrit$sr)*0.7),] %>% DF()
train_data_aa<-rbind(aa1,aa2)
aa3<-setdiff(data_exist$sr,aa1$sr)
test_existaa<-data1[aa3,] %>% DF()
aa4<-setdiff(data_attrit$sr,aa2$sr)
test_attritaa<- data1[aa4,] %>% DF()
test_data_aa<- rbind(test_existaa,test_attritaa)
test_data
##Logistic Regression .
logit_train<- glm(formula = Attrition_Flag ~
Dependent_count +
scale(Avg_Utilization_Ratio) +
scale(Total_Ct_Chng_Q4_Q1) +
scale(Total_Trans_Ct * Total_Trans_Amt) +
scale(Total_Revolving_Bal) +
scale(Months_Inactive_12_mon) +
scale(Contacts_Count_12_mon) +
Gender +
Income_Category +
Marital_Status +
scale(Total_Ct_Chng_Q4_Q1) +
log(Avg_Open_To_Buy) * Credit_Limit,
family = binomial(link = "logit"), data = train_data_aa)
summary(logit_train)
pred_model<- predict(logit_train,newdata = test_data_aa,type = "response")
qplot(pred_model)
plot(pred_model)
## Here we define any value less than 0.5 to be predicted as 0 and
##greater than 0.5 to be predicted as 1.
test_data_aa$pred[pred_model<=0.5]<-0
test_data_aa$pred[pred_model>0.5]<-1
table(a1=test_data_aa$pred,b=test_data_aa$Attrition_Flag)
## we factorise the data so as to use it in consfusion matrix.
test_pred<-test_data_aa$pred%>% factor()
test_AF<-as.factor(test_data_aa$Attrition_Flag)
confusionMatrix(test_pred,test_AF)
##Calculate accuracy.
mean(test_data_aa$pred==test_data_aa$Attrition_Flag)
F_meas(test_pred,test_AF)
recall(test_pred,test_AF)
precision(test_pred,test_AF)
##Here we use decision tree model
tree_model<-rpart(Attrition_Flag~.,data =train_data_aa,method = "class")
summary(tree_model)
#Then predicting the model using test data
pred_tree_model<-predict(tree_model,test_data_aa,type = "class")
table(pred_tree_model,test_data_aa$Attrition_Flag)
##calculating the accuracy.
mean(pred_tree_model==test_data_aa$Attrition_Flag)
confusionMatrix(pred_tree_model,test_AF)
confusionMatrix(pred_tree_model,test_AF)
##Using Random Forest model
my_forest<-randomForest(as.factor(Attrition_Flag)~.,data =traiin_data_aa,importance=TRUE,ntree=1000)
##Using Random Forest model
my_forest<-randomForest(as.factor(Attrition_Flag)~.,data =train_data_aa,importance=TRUE,ntree=1000)
##predicting model
predict_forest<- predict(my_forest,test_data_aa)
plot(predict_forest)
mean(predict_forest==test_data_aa$Attrition_Flag)
confusionMatrix(predict_forest,test_AF)
## Now using K-nearest neighbours
kn3<-knn3(as.factor(Attrition_Flag)~.,data =train_data_aa,k=7,prob=TRUE)
summary(kn3)
## we predict using kn3
kn3_predict<-predict(kn3,test_data_aa,type = "class")
plot(kn3_predict)
confusionMatrix(data = as.factor(kn3_predict), reference = test_AF)
library(ggthemes)
library(plotfunctions)
library(tidyr)
library(ggplot2)
library(moderndive)
library(skimr)
library(dplyr)
library(tidyverse)
library(readxl)
library(purrr)
library("TeachingDemos")
library("PerformanceAnalytics")
library(corrplot)
library(dummies)
library(caret)
library(generics)
library("e1071")
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(randomForest)
library(effects)
library(e1071)
#drop 3 colnames
data1<-BankChurners[,c(-22,-23,-1)]
colnames(data1)
## Assigning binary values
data1$Attrition_Flag[data1$Attrition_Flag=="Existing Customer"]<-0
data1$Attrition_Flag[data1$Attrition_Flag=="Attrited Customer"]<-1
#Assigning binary values to gender
data1$Gender[data1$Gender=="M"]<-0
data1$Gender[data1$Gender=="F"]<-1
##Naming and changing the class of some of columns
data1<- data1 %>%
summarise("Attrition_Flag"=as.numeric(Attrition_Flag),
Customer_Age,
"Gender"=as.factor(Gender),
"Dependent_count"=as.factor(Dependent_count),
Education_Level,
Marital_Status,
Income_Category,
Card_Category,
Months_on_book,
Total_Relationship_Count,
Months_Inactive_12_mon,
Contacts_Count_12_mon,
Credit_Limit,
Total_Revolving_Bal,
Avg_Open_To_Buy,
Total_Amt_Chng_Q4_Q1,
Total_Trans_Amt,
Total_Trans_Ct,
Total_Ct_Chng_Q4_Q1,
Avg_Utilization_Ratio)
##Adding "sr" in columns
data1$sr<- 1:nrow(data1)
colnames(data1)
#dividing the data into existed and attributed
data_exist<- data1 %>%
filter(Attrition_Flag==0)
data_attrit<- data1 %>%
filter(Attrition_Flag==1)
colnames(data1)
#colnames(data1) <- make.unique(names(data1))
DF<-function(DF, n=0) {
DF[rowSums(is.na(DF)) <= n,]
}
set.seed(000)
aa1<- data1[sample(data_exist$sr,length(data_exist$sr)*0.6),] %>% DF()
aa2<- data1[sample(data_attrit$sr,length(data_attrit$sr)*0.7),] %>% DF()
train_data_aa<-rbind(aa1,aa2)
aa3<-setdiff(data_exist$sr,aa1$sr)
test_existaa<-data1[aa3,] %>% DF()
aa4<-setdiff(data_attrit$sr,aa2$sr)
test_attritaa<- data1[aa4,] %>% DF()
test_data_aa<- rbind(test_existaa,test_attritaa)
##Logistic Regression .
logit_train<- glm(formula = Attrition_Flag ~
Dependent_count +
scale(Avg_Utilization_Ratio) +
scale(Total_Ct_Chng_Q4_Q1) +
scale(Total_Trans_Ct * Total_Trans_Amt) +
scale(Total_Revolving_Bal) +
scale(Months_Inactive_12_mon) +
scale(Contacts_Count_12_mon) +
Gender +
Income_Category +
Marital_Status +
scale(Total_Ct_Chng_Q4_Q1) +
log(Avg_Open_To_Buy) * Credit_Limit,
family = binomial(link = "logit"), data = train_data_aa)
summary(logit_train)
pred_model<- predict(logit_train,newdata = test_data_aa,type = "response")
qplot(pred_model)
plot(pred_model)
## Here we define any value less than 0.5 to be predicted as 0 and
##greater than 0.5 to be predicted as 1.
test_data_aa$pred[pred_model<=0.5]<-0
test_data_aa$pred[pred_model>0.5]<-1
table(a1=test_data_aa$pred,b=test_data_aa$Attrition_Flag)
## we factorise the data so as to use it in consfusion matrix.
test_pred<-test_data_aa$pred%>% factor()
test_AF<-as.factor(test_data_aa$Attrition_Flag)
confusionMatrix(test_pred,test_AF)
##Calculate accuracy.
mean(test_data_aa$pred==test_data_aa$Attrition_Flag)
F_meas(test_pred,test_AF)
recall(test_pred,test_AF)
precision(test_pred,test_AF)
##Here we use decision tree model
tree_model<-rpart(Attrition_Flag~.,data =train_data_aa,method = "class")
summary(tree_model)
##Plotting the tree model
plot(tree_model)
fancyRpartPlot(tree_model,sub = NA)
#Then predicting the model using test data
pred_tree_model<-predict(tree_model,test_data_aa,type = "class")
plot(pred_tree_model)
table(pred_tree_model,test_data_aa$Attrition_Flag)
##calculating the accuracy.
mean(pred_tree_model==test_data_aa$Attrition_Flag)
confusionMatrix(pred_tree_model,test_AF)
##Using Random Forest model
my_forest<-randomForest(as.factor(Attrition_Flag)~.,data =train_data_aa,importance=TRUE,ntree=1000)
#varImpPlot(my_forest)
##predicting model
predict_forest<- predict(my_forest,test_data_aa)
plot(predict_forest)
## calculating accuracy of the random forest model.
mean(predict_forest==test_data_aa$Attrition_Flag)
confusionMatrix(predict_forest,test_AF)
## Now using K-nearest neighbours
kn3<-knn3(as.factor(Attrition_Flag)~.,data =train_data_aa,k=7,prob=TRUE)
summary(kn3)
## we predict using kn3
kn3_predict<-predict(kn3,test_data_aa,type = "class")
plot(kn3_predict)
confusionMatrix(data = as.factor(kn3_predict), reference = test_AF)
##Calculating accuracy
mean(kn3_predict==test_data$Attrition_Flag)
###SVM model
# Fitting model SVM
svm_model <-svm(Attrition_Flag ~., data = train_data)
summary(svm_model)
#Predict Output
predicted_SVM<- predict(svm_model,test_data,type="class")
plot(predicted_SVM)
## Here we define any value less than 0.5 to be predicted as 0 and
##greater than 0.5 to be predicted as 1.
test_data$pred1[predicted_SVM<0.5]<-0
test_data$pred1[predicted_SVM>0.5]<-1
table(a1=test_data$pred1,b=test_data$Attrition_Flag)
##ACcuracy of data
mean(test_data$pred1==test_data$Attrition_Flag)
##Confusion Matrix of the model.
tp_svm<-test_data$pred1%>% factor()
confusionMatrix(data= tp_svm ,reference = as.factor(test_data$Attrition_Flag ))
data1$Dependent_count<-as.factor(Dependent_count)
##Reclassifying gender and dependent count
data1$Gender<- as.factor(Gender)
data1$Dependent_count<-as.factor(data1$Dependent_count)
##Reclassifying gender and dependent count
data1$Gender<- as.factor(data1$Gender)
